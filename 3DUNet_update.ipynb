{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def organize_braTS_data(src_root=\"./data\", dst_root=\"./dataset\"):\n",
    "    \"\"\"\n",
    "    Extract ./TumorSegmentationData to data and label form\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(dst_root, \"data\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dst_root, \"label\"), exist_ok=True)\n",
    "    \n",
    "    for folder in sorted(os.listdir(src_root)):\n",
    "        folder_path = os.path.join(src_root, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            case_number = folder.split(\"_\")[-1]  # Extracting case number\n",
    "            \n",
    "            for file in os.listdir(folder_path):\n",
    "                src_file = os.path.join(folder_path, file)\n",
    "                if file.endswith(\"_t1.nii\"):\n",
    "                    dst_file = os.path.join(dst_root, \"data\", f\"data_{case_number}_t1.nii\")\n",
    "                    shutil.move(src_file, dst_file)\n",
    "                elif file.endswith(\"_seg.nii\"):\n",
    "                    dst_file = os.path.join(dst_root, \"label\", f\"label_{case_number}.nii\")\n",
    "                    shutil.move(src_file, dst_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    organize_braTS_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU!\")\n",
    "EPOCHS = 100\n",
    "TRAIN_SPLIT = 0.8\n",
    "LR = 2e-3\n",
    "STEP_SIZE = 3\n",
    "GAMMA = 0.1\n",
    "MANUAL_SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "TARGET_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\"\n",
    "    모든 관련 라이브러리의 랜덤 시드를 고정합니다.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): 고정할 랜덤 시드 값\n",
    "    \"\"\"\n",
    "    # Python 내장 랜덤 시드 고정\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy 랜덤 시드 고정\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch 랜덤 시드 고정\n",
    "    torch.manual_seed(seed)  # CPU에서 PyTorch 랜덤 시드 고정\n",
    "    torch.cuda.manual_seed(seed)  # GPU에서 PyTorch 랜덤 시드 고정 (CUDA 장치)\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 GPU에서 PyTorch 랜덤 시드 고정\n",
    "    \n",
    "    # CuDNN 관련 설정\n",
    "    torch.backends.cudnn.deterministic = True  # 결정적 알고리즘을 사용하여 결과 고정\n",
    "    torch.backends.cudnn.benchmark = False  # 벤치마크 모드를 비활성화하여 성능 최적화 방지\n",
    "\n",
    "# 시드 고정 예시\n",
    "set_random_seed(MANUAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    scan = nib.load(filepath)\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min_val = -1000\n",
    "    max_val = 400\n",
    "    volume[volume < min_val] = min_val\n",
    "    volume[volume > max_val] = max_val\n",
    "    volume = (volume - min_val) / (max_val - min_val)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize(image, target_size):\n",
    "    \"\"\"이미지 크기 조정 (PIL 없이 numpy 기반)\"\"\"\n",
    "    if image.ndim == 2:\n",
    "        # 흑백 이미지 (H, W) 크기 조정\n",
    "        return scipy.ndimage.zoom(image, (target_size[0] / image.shape[0], target_size[1] / image.shape[1]), order=1)\n",
    "    elif image.ndim == 3:\n",
    "        # 컬러 이미지 (H, W, C) 크기 조정\n",
    "        zoom_factors = (target_size[0] / image.shape[0], target_size[1] / image.shape[1])\n",
    "        resized = np.stack([scipy.ndimage.zoom(image[..., i], zoom_factors, order=1) for i in range(image.shape[2])], axis=-1)\n",
    "        return resized\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image dimensions\")\n",
    "\n",
    "            \n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize to 64x64x64\"\"\"\n",
    "    desired_depth = TARGET_SIZE\n",
    "    desired_width = TARGET_SIZE\n",
    "    desired_height = TARGET_SIZE\n",
    "    \n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    \n",
    "    depth_factor = desired_depth / current_depth\n",
    "    width_factor = desired_width / current_width\n",
    "    height_factor = desired_height / current_height\n",
    "    \n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label_1.nii', 'label_2.nii', 'label_3.nii', 'label_4.nii', 'label_5.nii', 'label_6.nii', 'label_7.nii', 'label_8.nii', 'label_9.nii', 'label_10.nii', 'label_11.nii', 'label_12.nii', 'label_13.nii', 'label_14.nii', 'label_15.nii', 'label_16.nii', 'label_17.nii', 'label_18.nii', 'label_19.nii', 'label_20.nii', 'label_21.nii', 'label_22.nii', 'label_23.nii', 'label_24.nii']\n",
      "Missing files: ['label_1.nii', 'label_2.nii', 'label_3.nii', 'label_4.nii', 'label_5.nii', 'label_6.nii', 'label_7.nii', 'label_8.nii', 'label_9.nii', 'label_10.nii', 'label_11.nii', 'label_12.nii', 'label_13.nii', 'label_14.nii', 'label_15.nii', 'label_16.nii', 'label_17.nii', 'label_18.nii', 'label_19.nii', 'label_20.nii', 'label_21.nii', 'label_22.nii', 'label_23.nii', 'label_24.nii']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 경로 설정 (현재 작업 디렉토리 기준)\n",
    "data_dir = './dataset/label'\n",
    "\n",
    "# 숫자 001부터 369까지 파일이 있는지 확인 (_001, _002, ...)\n",
    "expected_files = [f\"label_{str(i)}.nii\" for i in range(1, 25)]\n",
    "print(expected_files)\n",
    "# 실제 파일 목록 가져오기\n",
    "try:\n",
    "    actual_files = sorted(os.listdir(data_dir))\n",
    "    # 실제 파일 목록에 있는지 확인\n",
    "    missing_files = [file for file in expected_files if file not in actual_files]\n",
    "\n",
    "    if missing_files:\n",
    "        print(\"Missing files:\", missing_files)\n",
    "    else:\n",
    "        print(\"All expected files are present.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The directory {data_dir} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir):\n",
    "        self.data_paths = sorted([f for f in os.listdir(data_dir) if f.endswith('.nii')])\n",
    "        self.label_paths = sorted([f for f in os.listdir(label_dir) if f.endswith('.nii')])\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "\n",
    "        # 데이터와 라벨의 개수가 일치하는지 확인\n",
    "        assert len(self.data_paths) == len(self.label_paths), \\\n",
    "            f\"Mismatch between data and label files: {len(self.data_paths)} | {len(self.label_paths)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_path = os.path.join(self.data_dir, self.data_paths[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_paths[idx])\n",
    "        \n",
    "        data = read_nifti_file(data_path)\n",
    "        label = read_nifti_file(label_path)     \n",
    "\n",
    "        # 최소, 최대 값 계산\n",
    "        # min_val = np.min(data)\n",
    "        # max_val = np.max(data)\n",
    "        # print(min_val, max_val)\n",
    "        data = normalize(data)\n",
    "        data = resize_volume(data)\n",
    "        label = resize_volume(label)\n",
    "\n",
    "        # Convert numpy to torch tensor\n",
    "        data = torch.tensor(data, dtype=torch.float32).unsqueeze(0)     # Add channel dimension\n",
    "        label = torch.tensor(label, dtype=torch.long)                   # For segmentation tasks, long type is needed for one-hot encoding\n",
    "    \n",
    "        # One-hot encoding: (64, 64, 64) -> (64, 64, 64, 5) -> (5, 64, 64, 64)\n",
    "        # label_one_hot = F.one_hot(label, num_classes=5).permute(3, 0, 1, 2).float()\n",
    "        label_one_hot = label\n",
    "        return data, label_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDataset(\"./dataset/data\", \"./dataset/label\")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  torch.Size([1, 32, 32, 32]) <class 'torch.Tensor'>\n",
      "label shape:  torch.Size([32, 32, 32]) <class 'torch.Tensor'>\n",
      "(32, 32, 32)\n",
      "torch.Size([32, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_sample(dataset, idx=0):\n",
    "    \"\"\"Visualize a single sample from the dataset\"\"\"\n",
    "    data, label = dataset[idx]\n",
    "    print(\"data shape: \", data.shape, type(data))\n",
    "    print(\"label shape: \", label.shape, type(label))\n",
    "    \n",
    "    data = data.squeeze().numpy()\n",
    "    # label = torch.argmax(label, dim=0)\n",
    "    \n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    slice_idx = data.shape[-1] // 2  # Select middle slice\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(data[:, :, slice_idx], cmap='gray')\n",
    "    axes[0].set_title(\"MRI Scan\")\n",
    "    \n",
    "    axes[1].imshow(label[:, :, slice_idx], cmap='jet')\n",
    "    axes[1].set_title(\"Segmentation Label\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(dataset, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\" (Conv3D -> BN -> ReLU) * 2 \"\"\"\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\" (Conv3D -> ReLU) * 2 \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.upconv = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upconv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, base_channels=32):\n",
    "        super(UNet3D, self).__init__()\n",
    "\n",
    "        self.encoder1 = DoubleConv(in_channels, base_channels, base_channels * 2)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder2 = DoubleConv(base_channels * 2, base_channels * 2, base_channels * 4)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder3 = DoubleConv(base_channels * 4, base_channels * 4, base_channels * 8)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder4 = DoubleConv(base_channels * 8, base_channels * 8, base_channels * 16)\n",
    "\n",
    "        self.upconv3 = UpConv(base_channels * 16, base_channels * 16)\n",
    "        self.decoder3 = DoubleConv(base_channels * 24, base_channels * 8, base_channels * 8)\n",
    "\n",
    "        self.upconv2 = UpConv(base_channels * 8, base_channels * 8)\n",
    "        self.decoder2 = DoubleConv(base_channels * 12, base_channels * 4, base_channels * 4)\n",
    "\n",
    "        self.upconv1 = UpConv(base_channels * 4, base_channels * 4)\n",
    "        self.decoder1 = DoubleConv(base_channels * 6, base_channels * 2, base_channels * 2)\n",
    "\n",
    "        self.outconv = nn.Conv3d(base_channels * 2, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        dec3 = self.upconv3(enc4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        out = self.outconv(dec1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "model = UNet3D(in_channels=1, out_channels=5) # Tumor 1,2,3,4 | 배경\n",
    "x = torch.randn(1, 1, 64, 64, 64)  # Example 3D volume\n",
    "output = model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([2.0202e-01, 5.1284e+01, 4.6823e+01, 2.7771e+02, 1.7859e+02],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋에서 클래스별 샘플 개수 계산\n",
    "labels = []  # 전체 데이터셋에서 라벨을 수집해야 함\n",
    "\n",
    "for _, label in train_loader:\n",
    "    # label = torch.argmax(label, dim=1)  # 원-핫 벡터를 클래스 인덱스로 변환\n",
    "    labels.extend(label.cpu().numpy().flatten())  # numpy 배열로 변환 후 리스트에 추가\n",
    "\n",
    "# 클래스별 샘플 개수 계산\n",
    "class_counts = Counter(labels)\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "# 가중치 계산: 총 샘플 수 / (클래스별 샘플 수 * 클래스 개수)\n",
    "class_weights = {cls: total_samples / (count * len(class_counts)) for cls, count in class_counts.items()}\n",
    "class_weights = torch.tensor(list(class_weights.values()), dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jjpark/SKKAI/3D-Tumor-Segmentation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#현재 폴더 경로; 작업 폴더 기준\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/15 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 11.65 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 11.32 GiB is allocated by PyTorch, and 60.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 116\u001b[0m\n\u001b[1;32m    112\u001b[0m data, label \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), label\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 116\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m    119\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 73\u001b[0m, in \u001b[0;36mUNet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder3(dec3)\n\u001b[1;32m     72\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv2(dec3)\n\u001b[0;32m---> 73\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2(dec2)\n\u001b[1;32m     76\u001b[0m dec1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv1(dec2)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 11.65 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 11.32 GiB is allocated by PyTorch, and 60.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, verbose=True):\n",
    "        \"\"\"\n",
    "        Early stopping to stop training when validation loss doesn't improve.\n",
    "        \n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last improvement\n",
    "            min_delta (float): Minimum change to qualify as an improvement\n",
    "            verbose (bool): If True, prints a message when early stopping triggers\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Check if training should be stopped.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): Current validation loss\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            # Validation loss has improved\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss has not improved\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'Early stopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print('Early stopping triggered')\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "# Dataset splitting\n",
    "dataset_size = len(train_dataset)\n",
    "val_size = int(dataset_size * 0.2)  \n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train_subset, val_subset = random_split(\n",
    "    train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42) \n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS if 'NUM_WORKERS' in globals() else 4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS if 'NUM_WORKERS' in globals() else 4\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "# Initialize tracking variables\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=0.001, verbose=True)\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / total\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for data, label in progress_bar:\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = calculate_accuracy(output, label)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=f\"{acc:.4f}\")\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = train_acc / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{EPOCHS}\")\n",
    "        for data, label in val_progress_bar:\n",
    "            data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            acc = calculate_accuracy(output, label)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc\n",
    "            \n",
    "            val_progress_bar.set_postfix(loss=loss.item(), acc=f\"{acc:.4f}\")\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_acc / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save model if it's the best so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './model/3DUNet_best.pth')\n",
    "        print(f\"✓ Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss):\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "        \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation metrics for actual epochs trained\n",
    "epochs_trained = len(train_losses)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs_trained+1), train_losses, 'b-', label='Training')\n",
    "plt.plot(range(1, epochs_trained+1), val_losses, 'r-', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs_trained+1), train_accs, 'b-', label='Training')\n",
    "plt.plot(range(1, epochs_trained+1), val_accs, 'r-', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Loading best model for visualization...\")\n",
    "model.load_state_dict(torch.load('./model/3DUNet_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "sample_data, sample_label = next(iter(val_loader))\n",
    "sample_data, sample_label = sample_data.to(DEVICE), sample_label.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output = model(sample_data)\n",
    "    _, predicted = torch.max(sample_output, 1)\n",
    "\n",
    "def visualize_3d_predictions(sample_idx=0, num_slices=3):\n",
    "    \"\"\"\n",
    "    Visualize predictions on a 3D volume\n",
    "    \"\"\"\n",
    "    input_vol = sample_data[sample_idx, 0].cpu().numpy()  \n",
    "    label_vol = sample_label[sample_idx].cpu().numpy()\n",
    "    pred_vol = predicted[sample_idx].cpu().numpy()\n",
    "    \n",
    "    d, h, w = input_vol.shape\n",
    "    \n",
    "    z_indices = np.linspace(d//4, 3*d//4, num_slices, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_slices, 3, figsize=(15, 5*num_slices))\n",
    "    \n",
    "    if num_slices == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, z in enumerate(z_indices):\n",
    "        axes[i, 0].imshow(input_vol[z], cmap='gray')\n",
    "        axes[i, 0].set_title(f'Input (Slice {z})')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(label_vol[z], cmap='viridis')\n",
    "        axes[i, 1].set_title(f'Ground Truth (Slice {z})')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_vol[z], cmap='viridis')\n",
    "        axes[i, 2].set_title(f'Prediction (Slice {z})')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/3d_prediction_results.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "visualize_3d_predictions(sample_idx=0, num_slices=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: torch.Size([16, 1, 32, 32, 32])\n",
      "Original label shape: torch.Size([16, 32, 32, 32])\n",
      "Model output shape: torch.Size([16, 5, 32, 32, 32])\n",
      "Warning: Possible dimension mismatch in label tensor\n",
      "Processed label shape: torch.Size([16, 32, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 1it [00:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 Dice score: 0.3471\n",
      "\n",
      "Prediction statistics:\n",
      "Classes: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Counts: tensor([27676,  2534,  1748,   168,   642], device='cuda:0')\n",
      "Total voxels: 32768\n",
      "\n",
      "Ground truth statistics:\n",
      "Classes: tensor([ 0, 17, 18, 19, 20, 21, 22], device='cuda:0')\n",
      "Counts: tensor([915,  18,  43,  28,   8,  10,   2], device='cuda:0')\n",
      "Total voxels: 1024\n",
      "\n",
      "Tensor shapes:\n",
      "Original: torch.Size([32, 32, 32])\n",
      "Prediction: torch.Size([32, 32, 32])\n",
      "Ground truth: torch.Size([32, 32])\n",
      "Saved visualization files: original_0_0.nii, pred_0_0.nii, label_0_0.nii\n",
      "Sample 1 Dice score: 0.2385\n",
      "Sample 2 Dice score: 0.1067\n",
      "Sample 3 Dice score: 0.1484\n",
      "Sample 4 Dice score: 0.2491\n",
      "Sample 5 Dice score: 0.2451\n",
      "Sample 6 Dice score: 0.2169\n",
      "Sample 7 Dice score: 0.1846\n",
      "Sample 8 Dice score: 0.1620\n",
      "Sample 9 Dice score: 0.1920\n",
      "Sample 10 Dice score: 0.3016\n",
      "Sample 11 Dice score: 0.4062\n",
      "Sample 12 Dice score: 0.2519\n",
      "Sample 13 Dice score: 0.1395\n",
      "Sample 14 Dice score: 0.1943\n",
      "Sample 15 Dice score: 0.2204\n",
      "Original data shape: torch.Size([16, 1, 32, 32, 32])\n",
      "Original label shape: torch.Size([16, 32, 32, 32])\n",
      "Model output shape: torch.Size([16, 5, 32, 32, 32])\n",
      "Warning: Possible dimension mismatch in label tensor\n",
      "Processed label shape: torch.Size([16, 32, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 2it [00:02,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 Dice score: 0.3619\n",
      "Sample 1 Dice score: 0.1901\n",
      "Sample 2 Dice score: 0.0939\n",
      "Sample 3 Dice score: 0.3685\n",
      "Sample 4 Dice score: 0.0567\n",
      "Sample 5 Dice score: 0.2732\n",
      "Sample 6 Dice score: 0.1444\n",
      "Sample 7 Dice score: 0.4331\n",
      "Sample 8 Dice score: 0.0162\n",
      "Sample 9 Dice score: 0.1260\n",
      "Sample 10 Dice score: 0.2749\n",
      "Sample 11 Dice score: 0.2156\n",
      "Sample 12 Dice score: 0.2682\n",
      "Sample 13 Dice score: 0.1659\n",
      "Sample 14 Dice score: 0.2975\n",
      "Sample 15 Dice score: 0.2899\n",
      "Original data shape: torch.Size([16, 1, 32, 32, 32])\n",
      "Original label shape: torch.Size([16, 32, 32, 32])\n",
      "Model output shape: torch.Size([16, 5, 32, 32, 32])\n",
      "Warning: Possible dimension mismatch in label tensor\n",
      "Processed label shape: torch.Size([16, 32, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 3it [00:04,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 Dice score: 0.2823\n",
      "Sample 1 Dice score: 0.2684\n",
      "Sample 2 Dice score: 0.1349\n",
      "Sample 3 Dice score: 0.2605\n",
      "Sample 4 Dice score: 0.1028\n",
      "Sample 5 Dice score: 0.3676\n",
      "Sample 6 Dice score: 0.5626\n",
      "Sample 7 Dice score: 0.1340\n",
      "Sample 8 Dice score: 0.1432\n",
      "Sample 9 Dice score: 0.3568\n",
      "Sample 10 Dice score: 0.4031\n",
      "Sample 11 Dice score: 0.3838\n",
      "Sample 12 Dice score: 0.1049\n",
      "Sample 13 Dice score: 0.2837\n",
      "Sample 14 Dice score: 0.4046\n",
      "Sample 15 Dice score: 0.2911\n",
      "Original data shape: torch.Size([16, 1, 32, 32, 32])\n",
      "Original label shape: torch.Size([16, 32, 32, 32])\n",
      "Model output shape: torch.Size([16, 5, 32, 32, 32])\n",
      "Warning: Possible dimension mismatch in label tensor\n",
      "Processed label shape: torch.Size([16, 32, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 4it [00:05,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 Dice score: 0.0941\n",
      "Sample 1 Dice score: 0.1835\n",
      "Sample 2 Dice score: 0.3144\n",
      "Sample 3 Dice score: 0.0620\n",
      "Sample 4 Dice score: 0.1273\n",
      "Sample 5 Dice score: 0.2450\n",
      "Sample 6 Dice score: 0.1914\n",
      "Sample 7 Dice score: 0.0386\n",
      "Sample 8 Dice score: 0.1376\n",
      "Sample 9 Dice score: 0.0919\n",
      "Sample 10 Dice score: 0.2300\n",
      "Sample 11 Dice score: 0.1653\n",
      "Sample 12 Dice score: 0.1363\n",
      "Sample 13 Dice score: 0.2128\n",
      "Sample 14 Dice score: 0.2311\n",
      "Sample 15 Dice score: 0.1669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 5it [00:06,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: torch.Size([10, 1, 32, 32, 32])\n",
      "Original label shape: torch.Size([10, 32, 32, 32])\n",
      "Model output shape: torch.Size([10, 5, 32, 32, 32])\n",
      "Warning: Possible dimension mismatch in label tensor\n",
      "Processed label shape: torch.Size([10, 32, 32, 32])\n",
      "Sample 0 Dice score: 0.0519\n",
      "Sample 1 Dice score: 0.3020\n",
      "Sample 2 Dice score: 0.0451\n",
      "Sample 3 Dice score: 0.1432\n",
      "Sample 4 Dice score: 0.1297\n",
      "Sample 5 Dice score: 0.1881\n",
      "Sample 6 Dice score: 0.1846\n",
      "Sample 7 Dice score: 0.1809\n",
      "Sample 8 Dice score: 0.2642\n",
      "Sample 9 Dice score: 0.2621\n",
      "\n",
      "Test Results:\n",
      "Test Dice Accuracy: 0.2168\n",
      "Average Test Loss: 1.2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import nibabel as nib\n",
    "\n",
    "def save_as_nifti(tensor, filename):\n",
    "    \"\"\"\n",
    "    Save a tensor as a NIfTI file.\n",
    "    tensor: Tensor to be saved as NIfTI.\n",
    "    filename: The output file name.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array (NIfTI requires numpy arrays)\n",
    "    nifti_img = nib.Nifti1Image(tensor, affine=np.eye(4))  # Using identity matrix as affine\n",
    "    \n",
    "    # Save as NIfTI file\n",
    "    nib.save(nifti_img, filename)\n",
    "\n",
    "def multiclass_dice_coeff(input, target, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Dice Score for multi-class segmentation.\n",
    "    \n",
    "    Args:\n",
    "        input (torch.Tensor): Model output logits (C, H, W, D).\n",
    "        target (torch.Tensor): Ground truth labels (C, H, W, D).\n",
    "        epsilon (float): Small value to avoid division by zero.\n",
    "        \n",
    "    Returns:\n",
    "        mean_dice (float): Mean Dice Score across all classes.\n",
    "    \"\"\"\n",
    "    input = torch.argmax(input, dim=0)  # (H, W, D) - get class index with highest logit\n",
    "    \n",
    "    # Ensure target is long type\n",
    "    target = target.int()\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (input * target).sum()\n",
    "    union = input.sum() + target.sum()\n",
    "\n",
    "    # Compute Dice score\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "\n",
    "    return dice\n",
    "\n",
    "# Test loop\n",
    "save = False\n",
    "total_dice = 0\n",
    "total_samples = 0\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, label) in tqdm(enumerate(test_loader), desc=\"Testing\"):\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "        \n",
    "        data_clone = data.clone()\n",
    "        label_clone = label.clone()\n",
    "\n",
    "        # Check data dimensions before operations\n",
    "        print(f\"Original data shape: {data.shape}\")\n",
    "        print(f\"Original label shape: {label.shape}\")\n",
    "        \n",
    "        # Model forward pass\n",
    "        output = model(data)  # Output shape should be (batch, C, D, H, W)\n",
    "        print(f\"Model output shape: {output.shape}\")\n",
    "        \n",
    "        # Convert label for loss calculation - FIXED DIMENSION ISSUE\n",
    "        # Based on the error, we need to ensure label_clone has the shape (batch, D, H, W)\n",
    "        if label_clone.dim() == 5:  # If one-hot encoded with shape (batch, C, D, H, W)\n",
    "            label_clone = torch.argmax(label_clone, dim=1)  # Convert to class indices (batch, D, H, W)\n",
    "        elif label_clone.dim() == 4 and label_clone.shape != output.shape[2:]:\n",
    "            # Handle possible dimension mismatch\n",
    "            # For 3D UNet, ensure target has 3 spatial dimensions\n",
    "            print(\"Warning: Possible dimension mismatch in label tensor\")\n",
    "            \n",
    "        print(f\"Processed label shape: {label_clone.shape}\")\n",
    "        \n",
    "        # Ensure label has correct type for CrossEntropyLoss\n",
    "        label_clone = label_clone.long()\n",
    "        \n",
    "        # Compute loss - this line was causing the error\n",
    "        try:\n",
    "            loss = criterion(output, label_clone)\n",
    "            total_loss += loss.item()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in loss calculation: {e}\")\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label_clone.shape}\")\n",
    "            # Continue with visualization even if loss fails\n",
    "            loss = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        # Process each sample in the batch\n",
    "        for i in range(data.size(0)):\n",
    "            # Dice score calculation\n",
    "            try:\n",
    "                dice = multiclass_dice_coeff(output[i], label[i])\n",
    "                print(f\"Sample {i} Dice score: {dice.item():.4f}\")\n",
    "                total_dice += dice.item()\n",
    "                total_samples += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating Dice score: {e}\")\n",
    "                continue\n",
    "\n",
    "            # File names for saving\n",
    "            original_filename = f\"original_{batch_idx}_{i}.nii\"\n",
    "            pred_filename = f\"pred_{batch_idx}_{i}.nii\"\n",
    "            label_filename = f\"label_{batch_idx}_{i}.nii\"\n",
    "\n",
    "            # Process tensors for visualization\n",
    "            pred = torch.argmax(output[i], dim=0)  # (D, H, W) - class index with highest logit\n",
    "            original = data_clone[i, 0] if data_clone.shape[1] == 1 else torch.argmax(data_clone[i], dim=0)\n",
    "            gt_label = torch.argmax(label[i], dim=0) if label.shape[1] > 1 else label[i]\n",
    "\n",
    "            # Print stats for the first sample\n",
    "            if save == False:\n",
    "                print(\"\\nPrediction statistics:\")\n",
    "                unique_elements, counts = torch.unique(pred, return_counts=True)\n",
    "                print(f\"Classes: {unique_elements}\")\n",
    "                print(f\"Counts: {counts}\")\n",
    "                print(f\"Total voxels: {counts.sum()}\")\n",
    "                \n",
    "                print(\"\\nGround truth statistics:\")\n",
    "                unique_elements, counts = torch.unique(gt_label, return_counts=True)\n",
    "                print(f\"Classes: {unique_elements}\")\n",
    "                print(f\"Counts: {counts}\")\n",
    "                print(f\"Total voxels: {counts.sum()}\")\n",
    "                \n",
    "                print(f\"\\nTensor shapes:\")\n",
    "                print(f\"Original: {original.shape}\")\n",
    "                print(f\"Prediction: {pred.shape}\")\n",
    "                print(f\"Ground truth: {gt_label.shape}\")\n",
    "\n",
    "                # Save as NIfTI \n",
    "                save_as_nifti(original.cpu().numpy().astype(np.int16), original_filename)\n",
    "                save_as_nifti(pred.cpu().numpy().astype(np.int16), pred_filename)\n",
    "                save_as_nifti(gt_label.cpu().numpy().astype(np.int16), label_filename)\n",
    "\n",
    "                save = True\n",
    "                print(f\"Saved visualization files: {original_filename}, {pred_filename}, {label_filename}\")\n",
    "        \n",
    "# Compute average metrics\n",
    "if total_samples > 0:\n",
    "    avg_dice = total_dice / total_samples\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Test Dice Accuracy: {avg_dice:.4f}\")\n",
    "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
    "else:\n",
    "    print(\"No valid samples were processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 직접 0과 1을 선언한 5x5x5 입력 텐서\n",
    "input_tensor = torch.tensor([\n",
    "    [[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [0, 1, 0, 1, 0]],\n",
    "    [[1, 1, 0, 0, 1], [0, 1, 1, 0, 0], [1, 0, 1, 1, 1], [0, 0, 1, 0, 1], [1, 1, 0, 1, 0]],\n",
    "    [[0, 0, 1, 1, 0], [1, 0, 0, 1, 1], [0, 1, 1, 0, 0], [1, 1, 0, 1, 0], [0, 0, 1, 1, 1]],\n",
    "    [[1, 0, 1, 0, 1], [0, 1, 0, 1, 0], [1, 1, 0, 1, 1], [0, 1, 1, 0, 0], [1, 0, 0, 1, 1]],\n",
    "    [[0, 1, 1, 0, 0], [1, 0, 1, 1, 0], [0, 0, 1, 0, 1], [1, 1, 0, 1, 0], [0, 1, 1, 0, 1]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 직접 0과 1을 선언한 5x5x5 타겟 텐서\n",
    "target_tensor = torch.tensor([\n",
    "    [[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [0, 1, 0, 1, 0]],\n",
    "    [[1, 1, 0, 0, 1], [0, 1, 1, 0, 0], [1, 0, 1, 1, 1], [0, 0, 1, 0, 1], [1, 1, 0, 1, 0]],\n",
    "    [[0, 0, 1, 1, 0], [1, 0, 0, 1, 1], [0, 1, 1, 0, 0], [1, 1, 0, 1, 0], [0, 0, 1, 1, 1]],\n",
    "    [[1, 0, 1, 0, 1], [0, 1, 0, 1, 0], [1, 1, 0, 1, 1], [0, 1, 1, 0, 0], [1, 0, 0, 1, 1]],\n",
    "    [[0, 1, 1, 0, 0], [1, 0, 1, 1, 0], [0, 0, 1, 0, 1], [1, 1, 0, 1, 0], [0, 1, 1, 0, 1]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Dice coefficient 함수 정의\n",
    "def multiclass_dice_coeff(input, target, epsilon=1e-6):\n",
    "    intersection = (input * target).sum()\n",
    "    union = input.sum() + target.sum()\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "    return dice\n",
    "\n",
    "# Dice Score 계산\n",
    "dice_score = multiclass_dice_coeff(input_tensor, target_tensor)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Dice Score:\", dice_score.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
