{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU!\n"
     ]
    }
   ],
   "source": [
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU!\")\n",
    "EPOCHS = 25\n",
    "TRAIN_SPLIT = 0.8\n",
    "LR = 1e-2\n",
    "STEP_SIZE = 5\n",
    "GAMMA = 0.1\n",
    "MANUAL_SEED = 42\n",
    "BATCH_SIZE = 1\n",
    "CLASS = 5\n",
    "TARGET_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\"\n",
    "    모든 관련 라이브러리의 랜덤 시드를 고정합니다.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): 고정할 랜덤 시드 값\n",
    "    \"\"\"\n",
    "    # Python 내장 랜덤 시드 고정\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy 랜덤 시드 고정\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch 랜덤 시드 고정\n",
    "    torch.manual_seed(seed)  # CPU에서 PyTorch 랜덤 시드 고정\n",
    "    torch.cuda.manual_seed(seed)  # GPU에서 PyTorch 랜덤 시드 고정 (CUDA 장치)\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 GPU에서 PyTorch 랜덤 시드 고정\n",
    "    \n",
    "    # CuDNN 관련 설정\n",
    "    torch.backends.cudnn.deterministic = True  # 결정적 알고리즘을 사용하여 결과 고정\n",
    "    torch.backends.cudnn.benchmark = False  # 벤치마크 모드를 비활성화하여 성능 최적화 방지\n",
    "\n",
    "# 시드 고정 예시\n",
    "set_random_seed(MANUAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    scan = nib.load(filepath)\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min_val = -1000\n",
    "    max_val = 400\n",
    "    volume[volume < min_val] = min_val\n",
    "    volume[volume > max_val] = max_val\n",
    "    volume = (volume - min_val) / (max_val - min_val)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "# interpolation?\n",
    "def resize(image, target_size):\n",
    "    \"\"\"이미지 크기 조정 (PIL 없이 numpy 기반)\"\"\"\n",
    "    print(image.ndim)\n",
    "    # 이미지가 2D일 경우 (H, W)로 크기 조정\n",
    "    if image.ndim == 2:\n",
    "        return scipy.ndimage.zoom(image, (target_size[0] / image.shape[0], target_size[1] / image.shape[1]), order=1)\n",
    "    # 이미지가 3D일 경우 (H, W, C)로 크기 조정\n",
    "    elif image.ndim == 3:\n",
    "        return np.concatenate([scipy.ndimage.zoom(image[..., i], (target_size[0] / image.shape[0], target_size[1] / image.shape[1]), order=1)[:, :, np.newaxis] for i in range(image.shape[2])], axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image dimensions\")\n",
    "            \n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize to 64x64x64\"\"\"\n",
    "    desired_depth = TARGET_SIZE\n",
    "    desired_width = TARGET_SIZE\n",
    "    desired_height = TARGET_SIZE\n",
    "    \n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    \n",
    "    depth_factor = desired_depth / current_depth\n",
    "    width_factor = desired_width / current_width\n",
    "    height_factor = desired_height / current_height\n",
    "    \n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # 경로 설정 (현재 작업 디렉토리 기준)\n",
    "# data_dir = './dataset/label'\n",
    "\n",
    "# # 숫자 001부터 369까지 파일이 있는지 확인 (_001, _002, ...)\n",
    "# expected_files = [f\"label_{str(i).zfill(3)}.nii\" for i in range(1, 370)]\n",
    "\n",
    "# # 실제 파일 목록 가져오기\n",
    "# try:\n",
    "#     actual_files = sorted(os.listdir(data_dir))\n",
    "#     # 실제 파일 목록에 있는지 확인\n",
    "#     missing_files = [file for file in expected_files if file not in actual_files]\n",
    "\n",
    "#     if missing_files:\n",
    "#         print(\"Missing files:\", missing_files)\n",
    "#     else:\n",
    "#         print(\"All expected files are present.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"The directory {data_dir} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir):\n",
    "        self.data_paths = sorted([f for f in os.listdir(data_dir) if f.endswith('.nii')])\n",
    "        self.label_paths = sorted([f for f in os.listdir(label_dir) if f.endswith('.nii')])\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "\n",
    "        # 데이터와 라벨의 개수가 일치하는지 확인\n",
    "        assert len(self.data_paths) == len(self.label_paths), \\\n",
    "            f\"Mismatch between data and label files: {len(self.data_paths)} | {len(self.label_paths)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_path = os.path.join(self.data_dir, self.data_paths[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_paths[idx])\n",
    "        \n",
    "        data = read_nifti_file(data_path)\n",
    "        label = read_nifti_file(label_path)\n",
    "\n",
    "        # 최소, 최대 값 계산\n",
    "        # min_val = np.min(data)\n",
    "        # max_val = np.max(data)\n",
    "        # print(min_val, max_val)\n",
    "        data = normalize(data)\n",
    "        data = resize_volume(data)\n",
    "        label = resize_volume(label)\n",
    "\n",
    "        # Convert numpy to torch tensor\n",
    "        data = torch.tensor(data, dtype=torch.float32).unsqueeze(0)     # Add channel dimension\n",
    "        label = torch.tensor(label, dtype=torch.long)                   # For segmentation tasks, long type is needed for one-hot encoding\n",
    "\n",
    "        # One-hot encoding: (64, 64, 64) -> (64, 64, 64, 5) -> (5, 64, 64, 64)\n",
    "        label_one_hot = F.one_hot(label, num_classes=5).permute(3, 0, 1, 2).float()\n",
    "\n",
    "        return data, label_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDataset(\"./dataset/data\", \"./dataset/label\")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  torch.Size([1, 32, 32, 32]) <class 'torch.Tensor'>\n",
      "label shape:  torch.Size([5, 32, 32, 32]) <class 'torch.Tensor'>\n",
      "(32, 32, 32)\n",
      "torch.Size([32, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_sample(dataset, idx=0):\n",
    "    \"\"\"Visualize a single sample from the dataset\"\"\"\n",
    "    data, label = dataset[idx]\n",
    "    print(\"data shape: \", data.shape, type(data))\n",
    "    print(\"label shape: \", label.shape, type(label))\n",
    "    \n",
    "    data = data.squeeze().numpy()\n",
    "    label = torch.argmax(label, dim=0)\n",
    "    \n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    slice_idx = data.shape[-1] // 2  # Select middle slice\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(data[:, :, slice_idx], cmap='gray')\n",
    "    axes[0].set_title(\"MRI Scan\")\n",
    "    \n",
    "    axes[1].imshow(label[:, :, slice_idx], cmap='jet')\n",
    "    axes[1].set_title(\"Segmentation Label\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(dataset, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class SingleDeconv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super().__init__()\n",
    "        self.block = nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class SingleConv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=((kernel_size - 1) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Conv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleConv3DBlock(in_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Deconv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleDeconv3DBlock(in_planes, out_planes),\n",
    "            SingleConv3DBlock(out_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_size = int(embed_dim / num_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, self.all_head_size)\n",
    "        self.key = nn.Linear(embed_dim, self.all_head_size)\n",
    "        self.value = nn.Linear(embed_dim, self.all_head_size)\n",
    "\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.vis = False\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1()\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=786, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Torch linears have a `b` by default.\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, cube_size, patch_size, dropout):\n",
    "        super().__init__()\n",
    "        self.n_patches = int((cube_size[0] * cube_size[1] * cube_size[2]) / (patch_size * patch_size * patch_size))\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_embeddings = nn.Conv3d(in_channels=input_dim, out_channels=embed_dim,\n",
    "                                          kernel_size=patch_size, stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, cube_size, patch_size):\n",
    "        super().__init__()\n",
    "        self.attention_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mlp_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mlp_dim = int((cube_size[0] * cube_size[1] * cube_size[2]) / (patch_size * patch_size * patch_size))\n",
    "        self.mlp = PositionwiseFeedForward(embed_dim, 2048)\n",
    "        self.attn = SelfAttention(num_heads, embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "        h = x\n",
    "\n",
    "        x = self.mlp_norm(x)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, cube_size, patch_size, num_heads, num_layers, dropout, extract_layers):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(input_dim, embed_dim, cube_size, patch_size, dropout)\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.extract_layers = extract_layers\n",
    "        for _ in range(num_layers):\n",
    "            layer = TransformerBlock(embed_dim, num_heads, dropout, cube_size, patch_size)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, x):\n",
    "        extract_layers = []\n",
    "        hidden_states = self.embeddings(x)\n",
    "\n",
    "        for depth, layer_block in enumerate(self.layer):\n",
    "            hidden_states, _ = layer_block(hidden_states)\n",
    "            if depth + 1 in self.extract_layers:\n",
    "                extract_layers.append(hidden_states)\n",
    "\n",
    "        return extract_layers\n",
    "\n",
    "\n",
    "class UNETR(nn.Module):\n",
    "    def __init__(self, img_shape=(128, 128, 128), input_dim=4, output_dim=3, embed_dim=768, patch_size=16, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.img_shape = img_shape\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 12\n",
    "        self.ext_layers = [3, 6, 9, 12]\n",
    "\n",
    "        self.patch_dim = [int(x / patch_size) for x in img_shape]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer = \\\n",
    "            Transformer(\n",
    "                input_dim,\n",
    "                embed_dim,\n",
    "                img_shape,\n",
    "                patch_size,\n",
    "                num_heads,\n",
    "                self.num_layers,\n",
    "                dropout,\n",
    "                self.ext_layers\n",
    "            )\n",
    "\n",
    "        # U-Net Decoder\n",
    "        self.decoder0 = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(input_dim, 32, 3),\n",
    "                Conv3DBlock(32, 64, 3)\n",
    "            )\n",
    "\n",
    "        self.decoder3 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "                Deconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder6 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "            )\n",
    "\n",
    "        self.decoder9 = \\\n",
    "            Deconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder12_upsampler = \\\n",
    "            SingleDeconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder9_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(1024, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                SingleDeconv3DBlock(512, 256)\n",
    "            )\n",
    "\n",
    "        self.decoder6_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(512, 256),\n",
    "                Conv3DBlock(256, 256),\n",
    "                SingleDeconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder3_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(256, 128),\n",
    "                Conv3DBlock(128, 128),\n",
    "                SingleDeconv3DBlock(128, 64)\n",
    "            )\n",
    "\n",
    "        self.decoder0_header = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(128, 64),\n",
    "                Conv3DBlock(64, 64),\n",
    "                SingleConv3DBlock(64, output_dim, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.transformer(x)\n",
    "        z0, z3, z6, z9, z12 = x, *z\n",
    "        z3 = z3.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z6 = z6.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z9 = z9.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z12 = z12.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "\n",
    "        z12 = self.decoder12_upsampler(z12)\n",
    "        z9 = self.decoder9(z9)\n",
    "        z9 = self.decoder9_upsampler(torch.cat([z9, z12], dim=1))\n",
    "        z6 = self.decoder6(z6)\n",
    "        z6 = self.decoder6_upsampler(torch.cat([z6, z9], dim=1))\n",
    "        z3 = self.decoder3(z3)\n",
    "        z3 = self.decoder3_upsampler(torch.cat([z3, z6], dim=1))\n",
    "        z0 = self.decoder0(z0)\n",
    "        output = self.decoder0_header(torch.cat([z0, z3], dim=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "model = UNETR(img_shape=(32,32,32),input_dim=1, output_dim=5)\n",
    "x = torch.randn(1, 1, 32, 32, 32)  # Example 3D volume\n",
    "output = model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([2.0204e-01, 4.6540e+01, 2.7435e+02, 1.7721e+02, 5.1014e+01])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋에서 클래스별 샘플 개수 계산\n",
    "labels = []  # 전체 데이터셋에서 라벨을 수집해야 함\n",
    "for _, label in train_loader:\n",
    "    label = torch.argmax(label, dim=1)  # 원-핫 벡터를 클래스 인덱스로 변환\n",
    "    labels.extend(label.cpu().numpy().flatten())  # numpy 배열로 변환 후 리스트에 추가\n",
    "\n",
    "# 클래스별 샘플 개수 계산\n",
    "class_counts = Counter(labels)\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "# 가중치 계산: 총 샘플 수 / (클래스별 샘플 수 * 클래스 개수)\n",
    "class_weights = {cls: total_samples / (count * len(class_counts)) for cls, count in class_counts.items()}\n",
    "class_weights = torch.tensor(list(class_weights.values()), dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   1%|          | 2/294 [00:08<19:34,  4.02s/it, loss=2.54]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/adam.py:378\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    375\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)  # Reduce LR by 0.1 every 5 epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for data, label in progress_bar:\n",
    "        # extract data and label\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "        # reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # model output\n",
    "        output = model(data)\n",
    "\n",
    "        # change label shape to 1 x D x D x D \n",
    "        label = torch.argmax(label, dim=1, keepdim=True).squeeze(1)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # save model\n",
    "    torch.save(model.state_dict(), f'./model/3DUNet_{epoch+1}.pth')\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # learning rate scheduler step\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/8f5dczz52v56d7f3qlhdknp40000gn/T/ipykernel_3940/847289548.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"./model/3DUNet_20.pth\", map_location=torch.device(DEVICE))     # state_dict 저장\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load Model\n",
    "# model = UNet3D(in_channels=1, out_channels=5)\n",
    "\n",
    "# from torchinfo import summary\n",
    "# summary(model, input_size=(1,1,32,32,32))\n",
    "\n",
    "# state_dict = torch.load(\"./model/3DUNet_20.pth\", map_location=torch.device(DEVICE))     # state_dict 저장\n",
    "\n",
    "# model.load_state_dict(state_dict=state_dict)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 1it [00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4500260055065155\n",
      "tensor([0, 2, 3, 4])\n",
      "tensor([29570,  1534,   210,  1454])\n",
      "tensor(32768)\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([32475,   109,   135,    35,    14])\n",
      "tensor(32768)\n",
      "torch.Size([32, 32, 32])\n",
      "torch.Size([32, 32, 32])\n",
      "torch.Size([32, 32, 32])\n",
      "0.5124387145042419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 2it [00:01,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.430801659822464\n",
      "0.47349298000335693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 3it [00:01,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49966806173324585\n",
      "0.5094953179359436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 4it [00:02,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3880758583545685\n",
      "0.44439697265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 5it [00:02,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42423659563064575\n",
      "0.4808882474899292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 6it [00:03,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46087366342544556\n",
      "0.4926976263523102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 7it [00:03,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5002860426902771\n",
      "0.5105454325675964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 8it [00:04,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3986707627773285\n",
      "0.4498426914215088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 9it [00:04,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4604759216308594\n",
      "0.5150677561759949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 10it [00:05,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4822251498699188\n",
      "0.4156654179096222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 11it [00:05,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4962599277496338\n",
      "0.48092347383499146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 12it [00:06,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5142598152160645\n",
      "0.4924202263355255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 13it [00:06,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4929055869579315\n",
      "0.4325281083583832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 14it [00:07,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4771465063095093\n",
      "0.43641743063926697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 15it [00:07,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48720020055770874\n",
      "0.5012121200561523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16it [00:08,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5146638751029968\n",
      "0.4524051249027252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17it [00:08,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47806599736213684\n",
      "0.5219999551773071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 18it [00:09,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49770769476890564\n",
      "0.47391951084136963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 19it [00:09,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5058593153953552\n",
      "0.44439697265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 20it [00:10,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47256794571876526\n",
      "0.4826468527317047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 21it [00:10,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44907233119010925\n",
      "0.4618128836154938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 22it [00:11,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40665191411972046\n",
      "0.49594932794570923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 23it [00:11,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44147443771362305\n",
      "0.5195626616477966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 24it [00:12,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3909253776073456\n",
      "0.4358203113079071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 25it [00:12,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4792667329311371\n",
      "0.4204767346382141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 26it [00:13,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4777478277683258\n",
      "0.43249061703681946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 27it [00:13,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49918702244758606\n",
      "0.38485804200172424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 28it [00:14,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45685088634490967\n",
      "0.5205986499786377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 29it [00:14,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4812754988670349\n",
      "0.524595320224762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 30it [00:15,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5055185556411743\n",
      "0.49217742681503296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 31it [00:16,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45156413316726685\n",
      "0.4947056472301483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 32it [00:16,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.508104145526886\n",
      "0.4193338453769684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 33it [00:17,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47894906997680664\n",
      "0.5112221837043762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 34it [00:17,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4137096405029297\n",
      "0.4182274639606476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 35it [00:18,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45775875449180603\n",
      "0.49252426624298096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 36it [00:18,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4253345727920532\n",
      "0.4526609182357788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 37it [00:19,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4879680573940277\n",
      "0.4101693332195282\n",
      "Test Dice Accuracy: 0.4683\n",
      "Average Test Loss: 73.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "def save_as_nifti(tensor, filename):\n",
    "    \"\"\"\n",
    "    Save a tensor as a NIfTI file.\n",
    "    tensor: Tensor to be saved as NIfTI.\n",
    "    filename: The output file name.\n",
    "    \"\"\"\n",
    "    # 텐서를 numpy 배열로 변환 (NIfTI는 numpy 배열을 요구)\n",
    "    \n",
    "    # NIfTI 이미지로 변환 (Voxel 크기 정보가 필요할 경우, 적절히 설정할 수 있습니다)\n",
    "    nifti_img = nib.Nifti1Image(tensor, affine=np.eye(4))  # affine는 단위 행렬로 설정 (원하는 좌표계로 수정 가능)\n",
    "    \n",
    "    # NIfTI 파일로 저장\n",
    "    nib.save(nifti_img, filename)\n",
    "\n",
    "# def save_as_nifti_labelmap(tensor, filename):\n",
    "#     if isinstance(tensor, torch.Tensor):\n",
    "#         tensor = tensor.cpu().numpy()\n",
    "#     tensor = tensor.astype(np.uint8)\n",
    "    \n",
    "#     # 새로운 NIfTI 헤더 생성\n",
    "#     hdr = nib.Nifti1Header()\n",
    "#     hdr.set_data_dtype(np.uint8)\n",
    "    \n",
    "#     # 'label' intent를 설정하면 intent_code가 1002 (NIFTI_INTENT_LABEL)로 설정됩니다.\n",
    "#     hdr.set_intent('label', (), 'label')\n",
    "#     hdr['intent_name'] = b'LabelMap'\n",
    "#     hdr['descrip'] = b'LabelMap volume'\n",
    "    \n",
    "#     # 계산 값 최소/최대 (선택 사항)\n",
    "#     hdr['cal_min'] = 0\n",
    "#     hdr['cal_max'] = int(tensor.max())\n",
    "    \n",
    "#     # qform, sform 코드 설정 (기본 affine 정보를 의미)\n",
    "#     hdr['qform_code'] = 1\n",
    "#     hdr['sform_code'] = 1\n",
    "    \n",
    "#     # NIfTI 이미지 생성 (affine은 단위 행렬, 필요시 수정)\n",
    "#     nifti_img = nib.Nifti1Image(tensor, affine=np.eye(4), header=hdr)\n",
    "#     nib.save(nifti_img, filename)\n",
    "    \n",
    "\n",
    "def multiclass_dice_coeff(input, target, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Dice Score for multi-class segmentation.\n",
    "    \n",
    "    Args:\n",
    "        input (torch.Tensor): Model output logits (C, H, W, D).\n",
    "        target (torch.Tensor): Ground truth labels (C, H, W, D).\n",
    "        epsilon (float): Small value to avoid division by zero.\n",
    "        \n",
    "    Returns:\n",
    "        mean_dice (float): Mean Dice Score across all classes.\n",
    "    \"\"\"\n",
    "    input = torch.argmax(input, dim=0)  # (H, W, D) - get class index with highest logit\n",
    "    \n",
    "    # Ensure target is long type and convert to one-hot\n",
    "    target = target.int()\n",
    "    # target_one_hot = F.one_hot(target.argmax(dim=1), num_classes=num_classes).permute(3, 0, 1, 2).float()  # (C, H, W, D)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (input * target).sum()  # (C)\n",
    "    union = input.sum() + target.sum()  # (C)\n",
    "\n",
    "    # Compute Dice score per class\n",
    "    dice_per_class = (2. * intersection + epsilon) / (union + epsilon)  # (C)\n",
    "\n",
    "    # Compute mean Dice score across all classes\n",
    "    mean_dice = dice_per_class.mean()  # Scalar\n",
    "\n",
    "    return mean_dice\n",
    "\n",
    "\n",
    "\n",
    "# Test loop\n",
    "save = False\n",
    "total_dice = 0\n",
    "total_samples = 0\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, label) in tqdm(enumerate(test_loader), desc=\"Testing\"):\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "        \n",
    "        data_clone = data.clone()\n",
    "\n",
    "        label_clone = label.clone()\n",
    "\n",
    "        # change label shape to 1 x D x D x D \n",
    "        label_clone = torch.argmax(label_clone, dim=1, keepdim=True).squeeze(1)\n",
    "\n",
    "        output = model(data)  # Output shape: (batch, C, H, W, D)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, label_clone)  # CrossEntropyLoss expects (batch, C, H, W, D) and (batch, H, W, D)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Save predictions and labels as NIfTI files\n",
    "        for i in range(data.size(0)):  # Loop over batch\n",
    "\n",
    "            # dice score\n",
    "            dice = multiclass_dice_coeff(output[i], label[i]) #  C x D x D x D | C x D X D x D로 들어옴\n",
    "            print(dice.item())\n",
    "            total_dice += dice.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            original_filename = f\"original_{batch_idx}_{i}.nii\"\n",
    "            pred_filename = f\"pred_{batch_idx}_{i}.nii\"\n",
    "            label_filename = f\"label_{batch_idx}_{i}.nii\"\n",
    "\n",
    "            # Input: directly using logits, no need for softmax\n",
    "            pred = torch.argmax(output[i], dim=0)  # (H, W, D) - get class index with highest logit\n",
    "            original = torch.argmax(data_clone[i], dim=0)\n",
    "            # Label: Convert ground truth to class index\n",
    "            gt_label = torch.argmax(label[i], dim=0)\n",
    "\n",
    "            # check\n",
    "            # unique_elements, counts = torch.unique(gt_label, return_counts=True)\n",
    "            # print(unique_elements)\n",
    "            # print(counts)\n",
    "            # print(counts.sum())\n",
    "            # unique_elements, counts = torch.unique(pred, return_counts=True)\n",
    "            # print(unique_elements)\n",
    "            # print(counts)\n",
    "            # print(counts.sum())\n",
    "            \n",
    "            # print(pred.cpu().numpy().astype(np.int16))\n",
    "            \n",
    "            if save == False:\n",
    "                unique_elements, counts = torch.unique(pred, return_counts=True)\n",
    "                print(unique_elements)\n",
    "                print(counts)\n",
    "                print(counts.sum())\n",
    "                unique_elements, counts = torch.unique(gt_label, return_counts=True)\n",
    "                print(unique_elements)\n",
    "                print(counts)\n",
    "                print(counts.sum())\n",
    "                print(original.shape)\n",
    "                print(pred.shape)\n",
    "                print(gt_label.shape)\n",
    "                torch.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "                # Save as NIfTI \n",
    "                save_as_nifti(original.cpu().numpy().astype(np.int16), original_filename)\n",
    "                save_as_nifti(pred.cpu().numpy().astype(np.int16), pred_filename)\n",
    "                save_as_nifti(gt_label.cpu().numpy().astype(np.int16), label_filename)\n",
    "\n",
    "                save = True\n",
    "        \n",
    "# Compute average Dice Accuracy\n",
    "avg_dice = total_dice / total_samples\n",
    "avg_loss = total_loss / total_samples\n",
    "\n",
    "print(f\"Test Dice Accuracy: {avg_dice:.4f}\")\n",
    "print(f\"Average Test Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanIoU:\n",
    "    \"\"\"\n",
    "    Computes IoU for each class separately and then averages over all classes.\n",
    "\n",
    "    Args:\n",
    "        skip_background (bool): if True, background class (i.e. 0-label) will be skipped when computing IoU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, skip_background=True, **kwargs):\n",
    "        self.skip_background = skip_background\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        \"\"\"\n",
    "        :param input: 5D probability maps torch float tensor (NxCxDxHxW)\n",
    "        :param target: 4D or 5D ground truth torch tensor. 4D (NxDxHxW) tensor will be expanded to 5D as one-hot\n",
    "        :return: intersection over union averaged over all channels\n",
    "        \"\"\"\n",
    "        input_softmax = torch.softmax(input, dim=1)  # (2, 5, 64, 64, 64)\n",
    "\n",
    "        assert input.dim() == 5\n",
    "\n",
    "        n_classes = input.size(1)\n",
    "\n",
    "        if target.dim() == 4:\n",
    "            # convert input to segmentation\n",
    "            input = input.argmax(dim=1)\n",
    "\n",
    "        assert input.size() == target.size()\n",
    "\n",
    "        per_batch_iou = []\n",
    "        for _input, _target in zip(input, target):\n",
    "            # convert target to byte\n",
    "            _target = _target.byte()\n",
    "            per_channel_iou = []\n",
    "            start_idx = 0\n",
    "            # skip background only if target is 4D; for channel-wise computation (i.e. if target is 5D) we need to include it\n",
    "            if self.skip_background and target.dim() == 4:\n",
    "                start_idx = 1\n",
    "\n",
    "            for c in range(start_idx, n_classes):\n",
    "                if target.dim() == 5:\n",
    "                    iou = self._jaccard_index(_input[c] > 0.4, _target[c])\n",
    "                    per_channel_iou.append(iou)\n",
    "                else:\n",
    "                    iou = self._jaccard_index(_input == c, _target == c)\n",
    "                    per_channel_iou.append(iou)\n",
    "\n",
    "            assert per_channel_iou, \"All channels were ignored from the computation\"\n",
    "            mean_iou = torch.tensor(per_channel_iou).mean()\n",
    "            per_batch_iou.append(mean_iou)\n",
    "\n",
    "        return torch.tensor(per_batch_iou).mean()\n",
    "\n",
    "    def _jaccard_index(self, prediction, target):\n",
    "        \"\"\"\n",
    "        Computes IoU for a given target and prediction tensors\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8\n",
    "        intersection = torch.logical_and(target, prediction).sum()\n",
    "        union = torch.logical_or(target, prediction).sum()\n",
    "        return (intersection + epsilon) / (union + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 1it [00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0385)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 2it [00:01,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 3it [00:01,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0382)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 4it [00:02,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 5it [00:03,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 6it [00:03,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0378)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTesting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (2, 5, 64, 64, 64)\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mBraTSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_paths[idx])\n\u001b[1;32m     22\u001b[0m label_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_paths[idx])\n\u001b[0;32m---> 24\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_nifti_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m label \u001b[38;5;241m=\u001b[39m read_nifti_file(label_path)\n\u001b[1;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m normalize(data)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mread_nifti_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read and load volume\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m scan \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(filepath)\n\u001b[0;32m----> 4\u001b[0m scan \u001b[38;5;241m=\u001b[39m \u001b[43mscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scan\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/nibabel/dataobj_images.py:374\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[0;34m(self, caching, dtype)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/nibabel/arrayproxy.py:454\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/nibabel/arrayproxy.py:423\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    421\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 423\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scaled\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "total_dice = 0\n",
    "total_samples = 0\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, label) in tqdm(enumerate(test_loader), desc=\"Testing\"):\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "        data = torch.softmax(data, dim=1)  # (2, 5, 64, 64, 64)\n",
    "\n",
    "        # print(data.shape)\n",
    "        # print(label.shape)\n",
    "\n",
    "        output = model(data)  # Output shape: (batch, C, H, W, D)\n",
    "        \n",
    "        # IoU 계산\n",
    "        metric = MeanIoU(skip_background=True)\n",
    "        iou = metric(output, label)  # target을 byte로 변환\n",
    "        print(iou)\n",
    "        # Compute loss\n",
    "        loss = criterion(output, label_clone)  # CrossEntropyLoss expects (batch, C, H, W, D) and (batch, H, W, D)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # update accuracy\n",
    "        total_dice += iou\n",
    "        total_samples += 1\n",
    "        \n",
    "# Compute average Dice Accuracy\n",
    "avg_dice = total_dice / total_samples\n",
    "avg_loss = total_loss / total_samples\n",
    "\n",
    "print(f\"Test Dice Accuracy: {avg_dice:.4f}\")\n",
    "print(f\"Average Test Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
